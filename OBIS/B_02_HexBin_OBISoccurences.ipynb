{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf06b9c-fe9b-4394-b50c-c57164750307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /srv/conda/envs/notebook/lib/python3.11/site-packages/odp-0.0.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting h3\n",
      "  Using cached h3-3.7.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Using cached h3-3.7.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: h3\n",
      "Successfully installed h3-3.7.7\n"
     ]
    }
   ],
   "source": [
    "!pip install h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d510cdf4-2f80-4a47-aede-a39c24aef9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9482779a-1c9e-4132-9841-d36c7c2a425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23eaee27-342e-4a70-aa4f-0d1f5029c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'output_occurence_data_classified/obis_classified.csv'\n",
    "#output_file = f\"output_H3binned_data/OBIS_h3counts_res{resolution}.csv\"\n",
    "output_file_base = 'output_H3binned_data/OBIS_h3counts'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6fea618-73d1-42a4-a5ec-a2f9af0d463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>date_year</th>\n",
       "      <th>PrivateFlag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0c1cb7e9-c7d1-4643-b245-daa8839a183f</td>\n",
       "      <td>-3.135050</td>\n",
       "      <td>58.911830</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0c1cb7e9-c7d1-4643-b245-daa8839a183f</td>\n",
       "      <td>-6.887870</td>\n",
       "      <td>58.217500</td>\n",
       "      <td>2010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0c1cb7e9-c7d1-4643-b245-daa8839a183f</td>\n",
       "      <td>1.281997</td>\n",
       "      <td>58.690365</td>\n",
       "      <td>2006</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0c1cb7e9-c7d1-4643-b245-daa8839a183f</td>\n",
       "      <td>-1.104040</td>\n",
       "      <td>60.327460</td>\n",
       "      <td>2008</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0c1cb7e9-c7d1-4643-b245-daa8839a183f</td>\n",
       "      <td>1.744569</td>\n",
       "      <td>61.393000</td>\n",
       "      <td>2006</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>5bac0fd0-82cd-47df-bb58-a7fe226194ac</td>\n",
       "      <td>-74.289810</td>\n",
       "      <td>10.988967</td>\n",
       "      <td>2020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>2ae2a2bd-8412-405b-8a9f-b71adc41d4c5</td>\n",
       "      <td>-170.641190</td>\n",
       "      <td>-14.251471</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>38535a43-2982-4479-88c1-8303f7ee8665</td>\n",
       "      <td>-12.181700</td>\n",
       "      <td>42.155000</td>\n",
       "      <td>1985</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>0ec403c9-23b3-4529-ae51-8a0877f22450</td>\n",
       "      <td>-64.934380</td>\n",
       "      <td>18.374250</td>\n",
       "      <td>2019</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>2ae2a2bd-8412-405b-8a9f-b71adc41d4c5</td>\n",
       "      <td>-170.658840</td>\n",
       "      <td>-14.250376</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   dataset_id  decimalLongitude  \\\n",
       "0        0c1cb7e9-c7d1-4643-b245-daa8839a183f         -3.135050   \n",
       "1        0c1cb7e9-c7d1-4643-b245-daa8839a183f         -6.887870   \n",
       "2        0c1cb7e9-c7d1-4643-b245-daa8839a183f          1.281997   \n",
       "3        0c1cb7e9-c7d1-4643-b245-daa8839a183f         -1.104040   \n",
       "4        0c1cb7e9-c7d1-4643-b245-daa8839a183f          1.744569   \n",
       "...                                       ...               ...   \n",
       "9999995  5bac0fd0-82cd-47df-bb58-a7fe226194ac        -74.289810   \n",
       "9999996  2ae2a2bd-8412-405b-8a9f-b71adc41d4c5       -170.641190   \n",
       "9999997  38535a43-2982-4479-88c1-8303f7ee8665        -12.181700   \n",
       "9999998  0ec403c9-23b3-4529-ae51-8a0877f22450        -64.934380   \n",
       "9999999  2ae2a2bd-8412-405b-8a9f-b71adc41d4c5       -170.658840   \n",
       "\n",
       "         decimalLatitude  date_year  PrivateFlag  \n",
       "0              58.911830       2009        False  \n",
       "1              58.217500       2010        False  \n",
       "2              58.690365       2006        False  \n",
       "3              60.327460       2008        False  \n",
       "4              61.393000       2006        False  \n",
       "...                  ...        ...          ...  \n",
       "9999995        10.988967       2020        False  \n",
       "9999996       -14.251471       2016        False  \n",
       "9999997        42.155000       1985        False  \n",
       "9999998        18.374250       2019        False  \n",
       "9999999       -14.250376       2016        False  \n",
       "\n",
       "[10000000 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('output_occurence_data_classified/obis_classified.csv', nrows=10000000, dtype={'PrivateFlag': 'boolean', 'date_year': 'Int64'})\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c55b695-2c5a-4b57-88dc-508d3b1aabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_landoceanclass_level2 = pd.read_csv('../H3_LandOceanCoast_classification/h3_classification_level_2.csv')\n",
    "df_landoceanclass_level4 = pd.read_csv('../H3_LandOceanCoast_classification/h3_classification_level_4.csv')\n",
    "df_landoceanclass_level6 = pd.read_csv('../H3_LandOceanCoast_classification/h3_classification_level_6.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c436db38-e023-48d5-bd61-cd19cde75177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_landoceanclass = pd.concat([df_landoceanclass_level2, df_landoceanclass_level4, df_landoceanclass_level6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db790ce-8792-415f-8421-d8d0ef4370b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resolutions you want to calculate\n",
    "resolutions = [6, 4, 2]  # Start with the highest and work downwards\n",
    "\n",
    "# Chunk size for processing the large file\n",
    "chunksize = 10**6  # Adjust based on your memory capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba3f910-4bbf-4c7a-87a3-9019d1bb0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame for the highest resolution\n",
    "agg_hex_counts_highest = pd.DataFrame(columns=['h3_index', 'count', 'count_private'])\n",
    "\n",
    "# Initialize a counter to track the number of processed rows\n",
    "rows_processed = 0\n",
    "#total_rows = sum(1 for _ in open(input_file)) - 1  # Adjust if there's a header\n",
    "total_rows = 128621187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c70c87-a169-4204-a8c3-a89f3d7eb51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128621187"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e8a6ed-8b72-4f82-a2b7-2eb35f712be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 rows (0.78% complete)\n",
      "Processed 2000000 rows (1.55% complete)\n",
      "Processed 3000000 rows (2.33% complete)\n",
      "Processed 4000000 rows (3.11% complete)\n",
      "Processed 5000000 rows (3.89% complete)\n",
      "Processed 6000000 rows (4.66% complete)\n",
      "Processed 7000000 rows (5.44% complete)\n",
      "Processed 8000000 rows (6.22% complete)\n",
      "Processed 9000000 rows (7.00% complete)\n",
      "Processed 10000000 rows (7.77% complete)\n",
      "Processed 11000000 rows (8.55% complete)\n",
      "Processed 12000000 rows (9.33% complete)\n",
      "Processed 13000000 rows (10.11% complete)\n",
      "Processed 14000000 rows (10.88% complete)\n",
      "Processed 15000000 rows (11.66% complete)\n",
      "Processed 16000000 rows (12.44% complete)\n",
      "Processed 17000000 rows (13.22% complete)\n",
      "Processed 18000000 rows (13.99% complete)\n",
      "Processed 19000000 rows (14.77% complete)\n",
      "Processed 20000000 rows (15.55% complete)\n",
      "Processed 21000000 rows (16.33% complete)\n",
      "Processed 22000000 rows (17.10% complete)\n",
      "Processed 23000000 rows (17.88% complete)\n",
      "Processed 24000000 rows (18.66% complete)\n",
      "Processed 25000000 rows (19.44% complete)\n",
      "Processed 26000000 rows (20.21% complete)\n",
      "Processed 27000000 rows (20.99% complete)\n",
      "Processed 28000000 rows (21.77% complete)\n",
      "Processed 29000000 rows (22.55% complete)\n",
      "Processed 30000000 rows (23.32% complete)\n",
      "Processed 31000000 rows (24.10% complete)\n",
      "Processed 32000000 rows (24.88% complete)\n",
      "Processed 33000000 rows (25.66% complete)\n",
      "Processed 34000000 rows (26.43% complete)\n",
      "Processed 35000000 rows (27.21% complete)\n",
      "Processed 36000000 rows (27.99% complete)\n",
      "Processed 37000000 rows (28.77% complete)\n",
      "Processed 38000000 rows (29.54% complete)\n",
      "Processed 39000000 rows (30.32% complete)\n",
      "Processed 40000000 rows (31.10% complete)\n",
      "Processed 41000000 rows (31.88% complete)\n",
      "Processed 42000000 rows (32.65% complete)\n",
      "Processed 43000000 rows (33.43% complete)\n",
      "Processed 44000000 rows (34.21% complete)\n",
      "Processed 45000000 rows (34.99% complete)\n",
      "Processed 46000000 rows (35.76% complete)\n",
      "Processed 47000000 rows (36.54% complete)\n",
      "Processed 48000000 rows (37.32% complete)\n",
      "Processed 49000000 rows (38.10% complete)\n",
      "Processed 50000000 rows (38.87% complete)\n",
      "Processed 51000000 rows (39.65% complete)\n",
      "Processed 52000000 rows (40.43% complete)\n",
      "Processed 53000000 rows (41.21% complete)\n",
      "Processed 54000000 rows (41.98% complete)\n",
      "Processed 55000000 rows (42.76% complete)\n",
      "Processed 56000000 rows (43.54% complete)\n",
      "Processed 57000000 rows (44.32% complete)\n",
      "Processed 58000000 rows (45.09% complete)\n",
      "Processed 59000000 rows (45.87% complete)\n",
      "Processed 60000000 rows (46.65% complete)\n",
      "Processed 61000000 rows (47.43% complete)\n",
      "Processed 62000000 rows (48.20% complete)\n",
      "Processed 63000000 rows (48.98% complete)\n",
      "Processed 64000000 rows (49.76% complete)\n",
      "Processed 65000000 rows (50.54% complete)\n",
      "Processed 66000000 rows (51.31% complete)\n",
      "Processed 67000000 rows (52.09% complete)\n",
      "Processed 68000000 rows (52.87% complete)\n",
      "Processed 69000000 rows (53.65% complete)\n",
      "Processed 70000000 rows (54.42% complete)\n",
      "Processed 71000000 rows (55.20% complete)\n",
      "Processed 72000000 rows (55.98% complete)\n",
      "Processed 73000000 rows (56.76% complete)\n",
      "Processed 74000000 rows (57.53% complete)\n",
      "Processed 75000000 rows (58.31% complete)\n",
      "Processed 76000000 rows (59.09% complete)\n",
      "Processed 77000000 rows (59.87% complete)\n",
      "Processed 78000000 rows (60.64% complete)\n",
      "Processed 79000000 rows (61.42% complete)\n",
      "Processed 80000000 rows (62.20% complete)\n",
      "Processed 81000000 rows (62.98% complete)\n",
      "Processed 82000000 rows (63.75% complete)\n",
      "Processed 83000000 rows (64.53% complete)\n",
      "Processed 84000000 rows (65.31% complete)\n",
      "Processed 85000000 rows (66.09% complete)\n",
      "Processed 86000000 rows (66.86% complete)\n",
      "Processed 87000000 rows (67.64% complete)\n",
      "Processed 88000000 rows (68.42% complete)\n",
      "Processed 89000000 rows (69.20% complete)\n",
      "Processed 90000000 rows (69.97% complete)\n",
      "Processed 91000000 rows (70.75% complete)\n",
      "Processed 92000000 rows (71.53% complete)\n",
      "Processed 93000000 rows (72.31% complete)\n",
      "Processed 94000000 rows (73.08% complete)\n",
      "Processed 95000000 rows (73.86% complete)\n",
      "Processed 96000000 rows (74.64% complete)\n",
      "Processed 97000000 rows (75.42% complete)\n",
      "Processed 98000000 rows (76.19% complete)\n",
      "Processed 99000000 rows (76.97% complete)\n",
      "Processed 100000000 rows (77.75% complete)\n",
      "Processed 101000000 rows (78.53% complete)\n",
      "Processed 102000000 rows (79.30% complete)\n",
      "Processed 103000000 rows (80.08% complete)\n",
      "Processed 104000000 rows (80.86% complete)\n",
      "Processed 105000000 rows (81.64% complete)\n",
      "Processed 106000000 rows (82.41% complete)\n",
      "Processed 107000000 rows (83.19% complete)\n",
      "Processed 108000000 rows (83.97% complete)\n",
      "Processed 109000000 rows (84.74% complete)\n",
      "Processed 110000000 rows (85.52% complete)\n",
      "Processed 111000000 rows (86.30% complete)\n",
      "Processed 112000000 rows (87.08% complete)\n",
      "Processed 113000000 rows (87.85% complete)\n",
      "Processed 114000000 rows (88.63% complete)\n",
      "Processed 115000000 rows (89.41% complete)\n",
      "Processed 116000000 rows (90.19% complete)\n",
      "Processed 117000000 rows (90.96% complete)\n",
      "Processed 118000000 rows (91.74% complete)\n",
      "Processed 119000000 rows (92.52% complete)\n",
      "Processed 120000000 rows (93.30% complete)\n",
      "Processed 121000000 rows (94.07% complete)\n",
      "Processed 122000000 rows (94.85% complete)\n",
      "Processed 123000000 rows (95.63% complete)\n",
      "Processed 124000000 rows (96.41% complete)\n",
      "Processed 125000000 rows (97.18% complete)\n",
      "Processed 126000000 rows (97.96% complete)\n",
      "Processed 127000000 rows (98.74% complete)\n",
      "Processed 128000000 rows (99.52% complete)\n",
      "Processed 128621187 rows (100.00% complete)\n"
     ]
    }
   ],
   "source": [
    "# Process the file in chunks\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunksize, dtype={'PrivateFlag': 'boolean', 'date_year': 'Int64'}):\n",
    "    # Drop rows with missing values in the specified columns\n",
    "    chunk.dropna(subset=['decimalLatitude', 'decimalLongitude'], inplace=True)\n",
    "    \n",
    "    # Calculate H3 index for each row at the highest resolution (6)\n",
    "    chunk['h3_index'] = chunk.apply(lambda row: h3.geo_to_h3(row['decimalLatitude'], row['decimalLongitude'], 6), axis=1)\n",
    "    \n",
    "    # Group by H3 index to count the number of points in each hexagon\n",
    "    density = chunk.groupby('h3_index').agg(count=('h3_index', 'size'),\n",
    "                                            count_private=('PrivateFlag', lambda x: (x == True).sum())).reset_index()\n",
    "    \n",
    "    # Aggregate this chunk's counts with the overall counts for the highest resolution\n",
    "    agg_hex_counts_highest = pd.concat([agg_hex_counts_highest, density]).groupby('h3_index', as_index=False).sum()\n",
    "    \n",
    "    # Update the number of processed rows\n",
    "    rows_processed += len(chunk)\n",
    "    \n",
    "    # Calculate progress percentage\n",
    "    progress_percentage = (rows_processed / total_rows) * 100\n",
    "    tqdm.write(f\"Processed {rows_processed} rows ({progress_percentage:.2f}% complete)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca662eae-d695-48c9-9338-8e54e5be8664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to output_H3binned_data/OBIS_h3counts_res6.csv for resolution 6\n",
      "Data successfully written to output_H3binned_data/OBIS_h3counts_res4.csv for resolution 4\n",
      "Data successfully written to output_H3binned_data/OBIS_h3counts_res2.csv for resolution 2\n"
     ]
    }
   ],
   "source": [
    "# Store results for each resolution\n",
    "agg_hex_counts_by_res = {6: agg_hex_counts_highest}\n",
    "\n",
    "# Aggregate to lower resolutions\n",
    "for res in [4, 2]:\n",
    "    agg_hex_counts_by_res[res] = agg_hex_counts_by_res[6].copy()\n",
    "    agg_hex_counts_by_res[res]['h3_index'] = agg_hex_counts_by_res[6]['h3_index'].apply(lambda x: h3.h3_to_parent(x, res))\n",
    "    agg_hex_counts_by_res[res] = agg_hex_counts_by_res[res].groupby('h3_index', as_index=False).sum()\n",
    "\n",
    "# Merge each resolution's result with the classification data\n",
    "for res in resolutions:\n",
    "    agg_hex_counts_by_res[res] = pd.merge(agg_hex_counts_by_res[res], \n",
    "                                          df_landoceanclass[['h3_index', 'classification']], \n",
    "                                          on='h3_index', \n",
    "                                          how='left')\n",
    "\n",
    "    # Calculate the centroid of each hexagon with 5 decimal places\n",
    "    agg_hex_counts_by_res[res]['latitude'] = agg_hex_counts_by_res[res]['h3_index'].apply(lambda x: round(h3.h3_to_geo(x)[0], 5))\n",
    "    agg_hex_counts_by_res[res]['longitude'] = agg_hex_counts_by_res[res]['h3_index'].apply(lambda x: round(h3.h3_to_geo(x)[1], 5))\n",
    "    \n",
    "    # Write the aggregated DataFrame to a CSV file\n",
    "    output_file = f\"{output_file_base}_res{res}.csv\"\n",
    "    agg_hex_counts_by_res[res].to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Data successfully written to {output_file} for resolution {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ae2f3-ea61-43fe-af8c-271bc61c5c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bfbd0-9f59-458a-92a5-50ca2d0b3458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282a5a3-134e-4cda-b457-fb9ce513a056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34987e-d365-4057-b125-fe2e9b1308c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd48a0-fa38-4ef1-9f6e-e1122acffb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651d570-03f4-4659-8640-dde2a9008dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99efd2-436f-4915-b985-d8a6c77b9353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375be5bc-51ef-44a0-a8eb-b69d31782bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resolutions you want to calculate\n",
    "resolutions = [2, 4, 6]\n",
    "\n",
    "# Chunk size for processing the large file\n",
    "chunksize = 10**6  # Adjust based on your memory capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f1924-b3de-4426-a01d-695363c275c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ff459-7c4a-4827-8735-cb43bc586508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each resolution\n",
    "for resolution in resolutions:\n",
    "    print(f\"Processing resolution {resolution}...\")\n",
    "    \n",
    "    # Initialize an empty DataFrame for aggregation\n",
    "    agg_hex_counts = pd.DataFrame(columns=['h3_index', 'count', 'count_private'])\n",
    "    \n",
    "    # Initialize a counter to track the number of processed rows\n",
    "    rows_processed = 0\n",
    "    total_rows = sum(1 for _ in open(input_file)) - 1  # Adjust if there's a header\n",
    "    \n",
    "    # Process the file in chunks\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "        # Drop rows with missing values in the specified columns\n",
    "        chunk.dropna(subset=['decimalLatitude', 'decimalLongitude'], inplace=True)\n",
    "        \n",
    "        # Calculate H3 index for each row at the current resolution\n",
    "        chunk['h3_index'] = chunk.apply(lambda row: h3.geo_to_h3(row['decimalLatitude'], row['decimalLongitude'], resolution), axis=1)\n",
    "        \n",
    "        # Group by H3 index to count the number of points in each hexagon\n",
    "        density = chunk.groupby('h3_index').agg(count=('h3_index', 'size'),\n",
    "                                                count_private=('PrivateFlag', lambda x: (x == True).sum())).reset_index()\n",
    "        \n",
    "        # Aggregate this chunk's counts with the overall counts\n",
    "        agg_hex_counts = pd.concat([agg_hex_counts, density]).groupby('h3_index', as_index=False).sum()\n",
    "        \n",
    "        # Update the number of processed rows\n",
    "        rows_processed += len(chunk)\n",
    "        \n",
    "        # Calculate progress percentage\n",
    "        progress_percentage = (rows_processed / total_rows) * 100\n",
    "        tqdm.write(f\"Processed {rows_processed} rows ({progress_percentage:.2f}% complete)\")\n",
    "\n",
    "    # Merge the aggregated hex counts with the classification data\n",
    "    agg_hex_counts = pd.merge(agg_hex_counts, df_landoceanclass[['h3_index', 'classification']], on='h3_index', how='left')\n",
    "    \n",
    "    # Calculate the centroid of each hexagon\n",
    "    agg_hex_counts['latitude'] = agg_hex_counts['h3_index'].apply(lambda x: h3.h3_to_geo(x)[0])\n",
    "    agg_hex_counts['longitude'] = agg_hex_counts['h3_index'].apply(lambda x: h3.h3_to_geo(x)[1])\n",
    "    \n",
    "    # Write the aggregated DataFrame to a CSV file\n",
    "    output_file = f\"{output_file_base}_res_{resolution}.csv\"\n",
    "    agg_hex_counts.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Data successfully written to {output_file} for resolution {resolution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858ac9c-0943-44de-bce9-b3ddb2a97436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087c2de-b32d-494f-8f70-c13ce05b778a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21a0bc-d6d5-4991-8cc9-3d58a944d7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab6ba4-0327-46a4-9585-1b6ba6b194aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47574e88-6f7c-455f-a4cd-da4678741140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9336aa-f014-450f-92cc-4c631bf21860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049438fd-9ff6-4839-a2bf-b2c6435448b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c2c075-82c1-40d3-a255-5028319f36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the resolution of H3 hexagons\n",
    "resolution = 2\n",
    "\n",
    "# Define chunk size\n",
    "chunksize = 10**6  # Adjust this size according to your available memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b053de-0be1-454d-9fc0-a51d0fde4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "input_file = 'output_occurence_data_classified/obis_classified.csv'\n",
    "output_file = f\"output_H3binned_data/OBIS_h3counts_res{resolution}.csv\"\n",
    "\n",
    "total_rows = 128621187  # Approximate number of rows # NB PURELY FOR PROGRESS REPORTING, NOT ESSENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13d15fa-bf23-41ad-97ec-d9637faec1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "input_file = 'obis_classified_privateonly.csv'\n",
    "output_file = f\"output_H3binned_data/OBIS_h3counts_res{resolution}_privateonly.csv\"\n",
    "\n",
    "total_rows = 6440698  # Approximate number of rows # NB PURELY FOR PROGRESS REPORTING, NOT ESSENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805b6d1a-c4de-44c6-973f-58ba6b01fad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_H3binned_data/OBIS_h3counts_res2.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d95f5d-aa89-40b6-88b0-6dd5797a2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3995/143643203.py:7: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(input_file, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000000 rows (7.77% complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3995/143643203.py:7: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(input_file, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000000 rows (15.55% complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3995/143643203.py:7: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(input_file, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30000000 rows (23.32% complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3995/143643203.py:7: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(input_file, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40000000 rows (31.10% complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3995/143643203.py:7: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(input_file, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000000 rows (38.87% complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3995/143643203.py:7: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(input_file, chunksize=chunksize):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m chunk\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecimalLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecimalLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert latitude and longitude to H3 index\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mh3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeo_to_h3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecimalLatitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecimalLongitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Group by H3 index to count the number of points in each hexagon\u001b[39;00m\n\u001b[1;32m     15\u001b[0m density \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      9\u001b[0m chunk\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecimalLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecimalLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert latitude and longitude to H3 index\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: h3\u001b[38;5;241m.\u001b[39mgeo_to_h3(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecimalLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecimalLongitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, resolution), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Group by H3 index to count the number of points in each hexagon\u001b[39;00m\n\u001b[1;32m     15\u001b[0m density \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/pandas/core/series.py:1239\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m-> 1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame for aggregation\n",
    "agg_hex_counts = pd.DataFrame(columns=['h3_index', 'count'])\n",
    "\n",
    "# Initialize a counter to track the number of processed rows\n",
    "rows_processed = 0\n",
    "# Process the file in chunks\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "    # Drop rows with missing values in the specified columns\n",
    "    chunk.dropna(subset=['decimalLatitude', 'decimalLongitude'], inplace=True)\n",
    "    \n",
    "    # Convert latitude and longitude to H3 index\n",
    "    chunk['h3_index'] = chunk.apply(lambda row: h3.geo_to_h3(row['decimalLatitude'], row['decimalLongitude'], resolution), axis=1)\n",
    "    \n",
    "    # Group by H3 index to count the number of points in each hexagon\n",
    "    density = chunk['h3_index'].value_counts().reset_index()\n",
    "    density.columns = ['h3_index', 'count']\n",
    "    \n",
    "    # Aggregate this chunk's counts with the overall counts\n",
    "    agg_hex_counts = pd.concat([agg_hex_counts, density]).groupby('h3_index', as_index=False).sum()\n",
    "\n",
    "    # Update the number of processed rows\n",
    "    rows_processed += len(chunk)\n",
    "    \n",
    "    # Calculate progress percentage\n",
    "    progress_percentage = (rows_processed / total_rows) * 100\n",
    "    \n",
    "    # Print progress message\n",
    "    print(f\"Processed {rows_processed} rows ({progress_percentage:.2f}% complete)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104b91a-f644-450f-adb1-a5b1b01a0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of each hexagon\n",
    "agg_hex_counts['latitude'] = agg_hex_counts['h3_index'].apply(lambda x: h3.h3_to_geo(x)[0])\n",
    "agg_hex_counts['longitude'] = agg_hex_counts['h3_index'].apply(lambda x: h3.h3_to_geo(x)[1])\n",
    "\n",
    "# Write the aggregated DataFrame to a CSV file\n",
    "agg_hex_counts.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data successfully written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da3ac391-73dc-463a-9ce1-e5e16dca3669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3_index</th>\n",
       "      <th>count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82000ffffffffff</td>\n",
       "      <td>2</td>\n",
       "      <td>81.957492</td>\n",
       "      <td>31.757228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820017fffffffff</td>\n",
       "      <td>1</td>\n",
       "      <td>78.128155</td>\n",
       "      <td>51.704538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82001ffffffffff</td>\n",
       "      <td>126</td>\n",
       "      <td>81.029276</td>\n",
       "      <td>50.504635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82002ffffffffff</td>\n",
       "      <td>1</td>\n",
       "      <td>79.649682</td>\n",
       "      <td>22.599927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>820067fffffffff</td>\n",
       "      <td>22</td>\n",
       "      <td>80.953594</td>\n",
       "      <td>-6.231433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>82f377fffffffff</td>\n",
       "      <td>778</td>\n",
       "      <td>-72.680492</td>\n",
       "      <td>-151.356811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>82f387fffffffff</td>\n",
       "      <td>2</td>\n",
       "      <td>-80.953594</td>\n",
       "      <td>173.768567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>82f38ffffffffff</td>\n",
       "      <td>6</td>\n",
       "      <td>-81.905946</td>\n",
       "      <td>-168.639427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>82f3a7fffffffff</td>\n",
       "      <td>4589</td>\n",
       "      <td>-78.347546</td>\n",
       "      <td>174.341491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>82f3affffffffff</td>\n",
       "      <td>124</td>\n",
       "      <td>-79.313332</td>\n",
       "      <td>-172.567877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2729 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             h3_index  count   latitude   longitude\n",
       "0     82000ffffffffff      2  81.957492   31.757228\n",
       "1     820017fffffffff      1  78.128155   51.704538\n",
       "2     82001ffffffffff    126  81.029276   50.504635\n",
       "3     82002ffffffffff      1  79.649682   22.599927\n",
       "4     820067fffffffff     22  80.953594   -6.231433\n",
       "...               ...    ...        ...         ...\n",
       "2724  82f377fffffffff    778 -72.680492 -151.356811\n",
       "2725  82f387fffffffff      2 -80.953594  173.768567\n",
       "2726  82f38ffffffffff      6 -81.905946 -168.639427\n",
       "2727  82f3a7fffffffff   4589 -78.347546  174.341491\n",
       "2728  82f3affffffffff    124 -79.313332 -172.567877\n",
       "\n",
       "[2729 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hex = pd.read_csv(output_file)\n",
    "df_hex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
